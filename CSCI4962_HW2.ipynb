{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI4962-HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzX1wcWFCshk9il1uDUDxS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toccht/CSCI-4962/blob/main/CSCI4962_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO3dRfvi3WP0"
      },
      "source": [
        "# CSCI-4962 Homework 2\n",
        "Author: Trevor Tocchet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzz4n7H33ORJ",
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, StratifiedKFold, train_test_split, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier"
      ],
      "execution_count": 490,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "t_n5-YI0M5QS"
      },
      "source": [
        "#@title Disable Warnings\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 487,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0GsB-FXoAqM"
      },
      "source": [
        "#@title Data Pre-Processing\n",
        "df = pd.read_csv('banknote-data.csv')\n",
        "\n",
        "X = df.drop(df.columns[[4]], axis=1)\n",
        "Y = df.drop(df.columns[[0,1,2,3]], axis=1)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 494,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gMM6DKjFaTG",
        "outputId": "a700ee03-298b-484f-bd55-4af9d3e32842"
      },
      "source": [
        "#@title Decision Tree\n",
        "\n",
        "# Initialize our decision tree object\n",
        "classification_tree = tree.DecisionTreeClassifier(max_depth=None, min_samples_split=2, max_features=None, min_impurity_split=0)\n",
        "\n",
        "# Train our decision tree (tree induction + pruning)\n",
        "#classification_tree = classification_tree.fit(iris.data, iris.target)\n",
        "#tree.plot_tree(classification_tree)\n",
        "\n",
        "# evaluate the model\n",
        "cv = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "n_scores = cross_val_score(classification_tree, X, Y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
        "\n",
        "clf = classification_tree.fit(X, Y)\n",
        "print('Training set accuracy: %.3f ' % clf.score(X, Y) )\n",
        "\n",
        "classification_tree.fit(X_train, Y_train)\n",
        "dt_score = classification_tree.score(X_test, Y_test)\n",
        "print('Decision tree model accuracy with 0.7/0.3 split (Holdout Method): %.3f' % dt_score)"
      ],
      "execution_count": 507,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.982 (0.016)\n",
            "Training set accuracy: 1.000 \n",
            "Decision tree model accuracy with 0.7/0.3 split (Holdout Method): 0.976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAWtXXvsqGRq"
      },
      "source": [
        "Of the four hyperparameters (max_depth, min_samples_split, max_features, min_impurity_split) that I adjusted individually and at the same time, nothing resulted in increased accuracy versus the default parameters. If I changed the max_depth starting from one, the accuracy increased with each step until it leveled off at a max_depth around 6-7. For min_samples_split, the accuracy gradually decreased as the parameter increased. For max_features, there was no noticable affect on the accuracy. Lastly, min_impurity_split quickly decreased accuracy when the parameter was changed from 0.4 -> 0.5 but stayed relatively constant at the same accuraccy with larger inputs.\n",
        "\n",
        "Examples:\n",
        "\n",
        "(max_depth=None, min_samples_split=2, max_features=None, min_impurity_split=0) = 0.982\n",
        "\n",
        "(max_depth=None, min_samples_split=10000, max_features=None, min_impurity_split=0) = 0.555\n",
        "\n",
        "(max_depth=None, min_samples_split=2, max_features=None, min_impurity_split=0.4) = 0.852\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz_DilyhE0Nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3133bded-b0f0-4896-eb30-0996c32898b6"
      },
      "source": [
        "#@title Bagging (Random Forest)\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "\n",
        "# evaluate the model\n",
        "cv = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "n_scores = cross_val_score(random_forest, X, Y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n"
      ],
      "execution_count": 369,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.993 (0.008)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwNpnYLAFxdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e93aef0-fec2-485d-9049-343ceb17beb0"
      },
      "source": [
        "#@title Boosting (Gradient Boosting)\n",
        "\n",
        "gradient_boost = GradientBoostingClassifier()\n",
        "\n",
        "# evaluate the model\n",
        "cv = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
        "n_scores = cross_val_score(gradient_boost, X, Y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
      ],
      "execution_count": 502,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.996 (0.007)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxPAO3f9wu0u"
      },
      "source": [
        "The ensamble methods bagging and boosting both yielded high results but the boosting method preformed slightly better than bagging. Bagging involves averaging predictions made by many independent models that are fitted on different subsets of the same data. Boosting involves models that depend on the prediction errors of the previous models, building a \"strong-learner\" from many \"weak-learners.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IaTMAXELzhV",
        "outputId": "5d34114c-9ba9-4953-f0d1-aaed7a41de4c"
      },
      "source": [
        "#@title Metric Comparison\n",
        "\n",
        "k_fold = KFold(n_splits=5, random_state=1, shuffle=True) # default number of splits = 5\n",
        "\n",
        "random_forest.fit(X_train, Y_train)\n",
        "fr_score = random_forest.score(X_test, Y_test)\n",
        "print('Random forest model accuracy with 0.7/0.3 split (Holdout Method): %.3f' % fr_score)\n",
        "\n",
        "n_scores = cross_val_score(random_forest, X, Y, scoring='accuracy', cv=k_fold, n_jobs=-1, error_score='raise')\n",
        "print('Random forest model accuracy with 5 K-Fold: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
        "\n",
        "gradient_boost.fit(X_train, Y_train)\n",
        "gb_score = gradient_boost.score(X_test, Y_test)\n",
        "print('Gradient boosting model accuracy with 0.7/0.3 split (Holdout Method): %.3f' % gb_score)\n",
        "\n",
        "n_scores = cross_val_score(gradient_boost, X, Y, scoring='accuracy', cv=k_fold, n_jobs=-1, error_score='raise')\n",
        "print('Gradient boosting model accuracy with 5 K-Fold: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
        "\n",
        "\n",
        "print('\\nBalance of the dataset:\\n', df['class'].value_counts() , sep='' )\n"
      ],
      "execution_count": 506,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random forest model accuracy with 0.7/0.3 split (Holdout Method): 0.988\n",
            "Random forest model accuracy with 5 K-Fold: 0.991 (0.003)\n",
            "Gradient boosting model accuracy with 0.7/0.3 split (Holdout Method): 0.993\n",
            "Gradient boosting model accuracy with 5 K-Fold: 0.995 (0.002)\n",
            "\n",
            "Balance of the dataset:\n",
            "0    762\n",
            "1    610\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWq0Whjqa3Ws"
      },
      "source": [
        "Using the holdout method, there is a high variability in accuracy measure after different training rounds, especially in smaller datasets. This is because the model is being trained on a different split each time. This method also leads to the model being trained on only a subest of the entire training, leaving out data and patterns that the model could learn. This is where the high variability comes from because the accuracy is dependent on the training data set.\n",
        "\n",
        "K-Fold helps to eliminate this issue by repeating the holdout method k times for k subsets/splits. The model at each iteration is independent of the model in the previous iteration. So this accuracy metric is more reliable than that of the holdout method generally. Although, if there is an imbalance of data (more labels of a particular class than another class) then subsets could include an inaccurate representation of the whole dataset.\n",
        "\n",
        "Stratified K-Fold address this problem by restructuring the data so that each subset/split has a balanced and accurate representation of the entire dataset. This method leads to a trustworthy accuracy metric so that we can compare different models accurately.\n",
        "\n",
        "All these methods are non-exhaustive methods.\n",
        "\n",
        "Because (as shown above) there is a slight imbalance in the dataset, choosing to compare models with stratified k-fold method is the best choice. The decesion tree classifier had the lowest accuracy, followed by random forest, and gradient boosting had the best accuracy. \n",
        "\n",
        "More information/resources:\n",
        "\n",
        "\n",
        "*   https://www.analyticsvidhya.com/blog/2021/05/importance-of-cross-validation-are-evaluation-metrics-enough/\n",
        "*   https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right"
      ]
    }
  ]
}